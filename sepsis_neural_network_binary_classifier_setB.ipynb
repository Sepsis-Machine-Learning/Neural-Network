{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one easy way by using Pandas: (mean normalization)\n",
    "\n",
    "normalized_df=(df-df.mean())/df.std()\n",
    "\n",
    "#### or to use min-max normalization:\n",
    "\n",
    "normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "\n",
    "#### or use my own defined min-max normalization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process input data with pandas \n",
    "\n",
    "* #drop columns which have more than 94% NaN's (or less than 6% values) by index\n",
    "\n",
    "* col no.=7, EtCO2, 1, 0%\n",
    "* col no.=13, SaO2, 4572, 8%\n",
    "* col no.=14, AST, 3020, 5%\n",
    "* col no.=16, Alkalinephos, 2902, 5%\n",
    "* col no.=20, Bilirubin_direct, 224, 0%\n",
    "* col no.=22, Lactate, 3085, 5%\n",
    "* col no.=26, Bilirubin_total, 2581, 4%\n",
    "* col no.=27, TroponinI, 218, 0%\n",
    "* col no.=32, Fibrinogen, 812, 1%\n",
    "\n",
    "0s:38416\n",
    "1s:16090\n",
    "1s/0s ratio  = 41 %\n",
    "\n",
    "#### change HospAdmTime from negative to positive\n",
    "train.HospAdmTime = -1 * train.HospAdmTime\n",
    "#### replace NaN with mean\n",
    "train=train.fillna(train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34296, 41)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/setB_train.psv\", sep='|')\n",
    "print(train.shape)\n",
    "#print(train)\n",
    "#drop columns which have more than 94% NaN's by index\n",
    "#cols = [7,14,16,20,22,26,27,32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'HospAdmTime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ae30e992651d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cols = [7]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHospAdmTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHospAdmTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#replace NaN with mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'HospAdmTime'"
     ]
    }
   ],
   "source": [
    "#cols = [7]\n",
    "train.HospAdmTime = -1 * train.HospAdmTime\n",
    "train=train.drop(train.columns[cols],axis=1)\n",
    "#replace NaN with mean\n",
    "train[np.isnan(train)] = -99\n",
    "#train=train.fillna(train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.values\n",
    "test = pd.read_csv(\"../data/setB_test.psv\", sep='|')\n",
    "#train, test = train_test_split(train, test_size=0.2)\n",
    "#print(train.shape)\n",
    "#print(train)\n",
    "test.HospAdmTime = -1 * test.HospAdmTime\n",
    "test=test.drop(test.columns[cols],axis=1)\n",
    "#replace NaN with mean\n",
    "test[np.isnan(test)] = -99\n",
    "#test=test.fillna(test.mean())\n",
    "test=test.values\n",
    "\n",
    "#train, test = train_test_split(train, test_size=0.2)\n",
    "#print(train.shape)\n",
    "#print(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_raw = train[:, :-1] # for all but last column\n",
    "n_rows, n_cols = x_train_raw.shape\n",
    " \n",
    "x_test_raw = test[:, :-1]\n",
    "y_train = train[:, -1]  # for last column\n",
    "y_test = test[:, -1]  # for last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(n_cols,)),\n",
    "#    keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "])\n",
    "keras.optimizers.Adam(lr = 0.001)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01683105 0.0209048  0.01452616 ... 0.         0.02437714 0.01286449]\n",
      " [0.02069039 0.02036878 0.         ... 0.01061321 0.01089515 0.01415094]\n",
      " [0.01929674 0.02047599 0.         ... 0.01061321 0.01089515 0.01533019]\n",
      " ...\n",
      " [0.01993997 0.02111921 0.         ... 0.01061321 0.01636364 0.0155446 ]\n",
      " [0.02111921 0.02122642 0.         ... 0.         0.01061321 0.01104202]\n",
      " [0.01918954 0.02101201 0.         ... 0.         0.01061321 0.01275729]]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_train = NormalizeData(x_train_raw)\n",
    "x_test = NormalizeData(x_test_raw)\n",
    "#x_train = x_train_raw\n",
    "#x_test = x_test_raw\n",
    "print(x_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "34296/34296 [==============================] - 2s 55us/step - loss: 0.1777 - acc: 0.9325\n",
      "Epoch 2/200\n",
      "34296/34296 [==============================] - 2s 61us/step - loss: 0.1776 - acc: 0.9331\n",
      "Epoch 3/200\n",
      "34296/34296 [==============================] - 3s 86us/step - loss: 0.1776 - acc: 0.9328\n",
      "Epoch 4/200\n",
      "34296/34296 [==============================] - 3s 77us/step - loss: 0.1777 - acc: 0.9328\n",
      "Epoch 5/200\n",
      "34296/34296 [==============================] - 2s 71us/step - loss: 0.1775 - acc: 0.9324: 2\n",
      "Epoch 6/200\n",
      "34296/34296 [==============================] - 2s 49us/step - loss: 0.1774 - acc: 0.9324\n",
      "Epoch 7/200\n",
      "34296/34296 [==============================] - 2s 62us/step - loss: 0.1775 - acc: 0.9333: 0s - lo\n",
      "Epoch 8/200\n",
      "34296/34296 [==============================] - 3s 77us/step - loss: 0.1772 - acc: 0.9333\n",
      "Epoch 9/200\n",
      "34296/34296 [==============================] - 3s 76us/step - loss: 0.1773 - acc: 0.9336\n",
      "Epoch 10/200\n",
      "34296/34296 [==============================] - 2s 57us/step - loss: 0.1775 - acc: 0.9333\n",
      "Epoch 11/200\n",
      "34296/34296 [==============================] - 2s 59us/step - loss: 0.1773 - acc: 0.9329\n",
      "Epoch 12/200\n",
      "34296/34296 [==============================] - 2s 68us/step - loss: 0.1775 - acc: 0.9330\n",
      "Epoch 13/200\n",
      "34296/34296 [==============================] - 2s 64us/step - loss: 0.1773 - acc: 0.9325\n",
      "Epoch 14/200\n",
      "34296/34296 [==============================] - 2s 67us/step - loss: 0.1774 - acc: 0.9328: \n",
      "Epoch 15/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1775 - acc: 0.9332\n",
      "Epoch 16/200\n",
      "34296/34296 [==============================] - 2s 64us/step - loss: 0.1772 - acc: 0.9335\n",
      "Epoch 17/200\n",
      "34296/34296 [==============================] - 2s 73us/step - loss: 0.1774 - acc: 0.9331\n",
      "Epoch 18/200\n",
      "34296/34296 [==============================] - 2s 52us/step - loss: 0.1772 - acc: 0.9332\n",
      "Epoch 19/200\n",
      "34296/34296 [==============================] - 2s 51us/step - loss: 0.1772 - acc: 0.9333: 1s - \n",
      "Epoch 20/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1771 - acc: 0.9331\n",
      "Epoch 21/200\n",
      "34296/34296 [==============================] - 2s 62us/step - loss: 0.1772 - acc: 0.9334\n",
      "Epoch 22/200\n",
      "34296/34296 [==============================] - 2s 66us/step - loss: 0.1770 - acc: 0.9331\n",
      "Epoch 23/200\n",
      "34296/34296 [==============================] - 2s 56us/step - loss: 0.1773 - acc: 0.9330\n",
      "Epoch 24/200\n",
      "34296/34296 [==============================] - 3s 75us/step - loss: 0.1771 - acc: 0.9333\n",
      "Epoch 25/200\n",
      "34296/34296 [==============================] - 2s 71us/step - loss: 0.1772 - acc: 0.9335\n",
      "Epoch 26/200\n",
      "34296/34296 [==============================] - 4s 104us/step - loss: 0.1770 - acc: 0.9331\n",
      "Epoch 27/200\n",
      "34296/34296 [==============================] - 3s 85us/step - loss: 0.1773 - acc: 0.9327\n",
      "Epoch 28/200\n",
      "34296/34296 [==============================] - 3s 77us/step - loss: 0.1770 - acc: 0.9335\n",
      "Epoch 29/200\n",
      "34296/34296 [==============================] - 3s 76us/step - loss: 0.1770 - acc: 0.9333\n",
      "Epoch 30/200\n",
      "34296/34296 [==============================] - 2s 66us/step - loss: 0.1771 - acc: 0.9329\n",
      "Epoch 31/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1768 - acc: 0.9328\n",
      "Epoch 32/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1772 - acc: 0.9331\n",
      "Epoch 33/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1769 - acc: 0.9339\n",
      "Epoch 34/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1767 - acc: 0.9334\n",
      "Epoch 35/200\n",
      "34296/34296 [==============================] - 2s 58us/step - loss: 0.1769 - acc: 0.9333\n",
      "Epoch 36/200\n",
      "34296/34296 [==============================] - 2s 54us/step - loss: 0.1770 - acc: 0.9332\n",
      "Epoch 37/200\n",
      "34296/34296 [==============================] - 2s 54us/step - loss: 0.1770 - acc: 0.9327\n",
      "Epoch 38/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1768 - acc: 0.9334\n",
      "Epoch 39/200\n",
      "34296/34296 [==============================] - 2s 46us/step - loss: 0.1770 - acc: 0.9328\n",
      "Epoch 40/200\n",
      "34296/34296 [==============================] - 3s 77us/step - loss: 0.1769 - acc: 0.9337\n",
      "Epoch 41/200\n",
      "34296/34296 [==============================] - 3s 75us/step - loss: 0.1765 - acc: 0.9345: 0s - loss: 0.1774 - acc: 0.93\n",
      "Epoch 42/200\n",
      "34296/34296 [==============================] - 2s 58us/step - loss: 0.1769 - acc: 0.9337\n",
      "Epoch 43/200\n",
      "34296/34296 [==============================] - 2s 48us/step - loss: 0.1768 - acc: 0.9333\n",
      "Epoch 44/200\n",
      "34296/34296 [==============================] - 3s 87us/step - loss: 0.1769 - acc: 0.9332\n",
      "Epoch 45/200\n",
      "34296/34296 [==============================] - 3s 83us/step - loss: 0.1767 - acc: 0.9333\n",
      "Epoch 46/200\n",
      "34296/34296 [==============================] - 2s 56us/step - loss: 0.1766 - acc: 0.9337: 0s - loss: 0.171\n",
      "Epoch 47/200\n",
      "34296/34296 [==============================] - 2s 59us/step - loss: 0.1767 - acc: 0.9336\n",
      "Epoch 48/200\n",
      "34296/34296 [==============================] - 3s 83us/step - loss: 0.1767 - acc: 0.9332\n",
      "Epoch 49/200\n",
      "34296/34296 [==============================] - 2s 70us/step - loss: 0.1764 - acc: 0.9333\n",
      "Epoch 50/200\n",
      "34296/34296 [==============================] - 2s 60us/step - loss: 0.1766 - acc: 0.9333\n",
      "Epoch 51/200\n",
      "34296/34296 [==============================] - 2s 61us/step - loss: 0.1767 - acc: 0.9336\n",
      "Epoch 52/200\n",
      "34296/34296 [==============================] - 2s 51us/step - loss: 0.1765 - acc: 0.9334\n",
      "Epoch 53/200\n",
      "34296/34296 [==============================] - 2s 56us/step - loss: 0.1767 - acc: 0.9325\n",
      "Epoch 54/200\n",
      "34296/34296 [==============================] - 2s 57us/step - loss: 0.1765 - acc: 0.9343: 0s - loss: 0.1771 - acc: 0.9\n",
      "Epoch 55/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1765 - acc: 0.9340: 1s - \n",
      "Epoch 56/200\n",
      "34296/34296 [==============================] - 2s 56us/step - loss: 0.1766 - acc: 0.9340\n",
      "Epoch 57/200\n",
      "34296/34296 [==============================] - 3s 76us/step - loss: 0.1766 - acc: 0.9331\n",
      "Epoch 58/200\n",
      "34296/34296 [==============================] - 2s 68us/step - loss: 0.1766 - acc: 0.9330\n",
      "Epoch 59/200\n",
      "34296/34296 [==============================] - 2s 64us/step - loss: 0.1764 - acc: 0.9342: 1s - loss: 0.17 - ETA: 0s - loss: 0.1744 - a\n",
      "Epoch 60/200\n",
      "34296/34296 [==============================] - 2s 64us/step - loss: 0.1765 - acc: 0.9333\n",
      "Epoch 61/200\n",
      "34296/34296 [==============================] - 2s 60us/step - loss: 0.1765 - acc: 0.9338\n",
      "Epoch 62/200\n",
      "34296/34296 [==============================] - 2s 55us/step - loss: 0.1762 - acc: 0.9340\n",
      "Epoch 63/200\n",
      "34296/34296 [==============================] - 3s 81us/step - loss: 0.1763 - acc: 0.9333\n",
      "Epoch 64/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1763 - acc: 0.9337\n",
      "Epoch 65/200\n",
      "34296/34296 [==============================] - 2s 69us/step - loss: 0.1764 - acc: 0.9329\n",
      "Epoch 66/200\n",
      "34296/34296 [==============================] - 2s 55us/step - loss: 0.1762 - acc: 0.9342\n",
      "Epoch 67/200\n",
      "34296/34296 [==============================] - ETA: 0s - loss: 0.1761 - acc: 0.933 - 4s 110us/step - loss: 0.1764 - acc: 0.9337\n",
      "Epoch 68/200\n",
      "34296/34296 [==============================] - 4s 109us/step - loss: 0.1766 - acc: 0.9342\n",
      "Epoch 69/200\n",
      "34296/34296 [==============================] - 2s 59us/step - loss: 0.1762 - acc: 0.9341\n",
      "Epoch 70/200\n",
      "34296/34296 [==============================] - 4s 121us/step - loss: 0.1764 - acc: 0.9343\n",
      "Epoch 71/200\n",
      "34296/34296 [==============================] - 4s 119us/step - loss: 0.1762 - acc: 0.9338\n",
      "Epoch 72/200\n",
      "34296/34296 [==============================] - 4s 120us/step - loss: 0.1764 - acc: 0.9340\n",
      "Epoch 73/200\n",
      "34296/34296 [==============================] - 4s 117us/step - loss: 0.1761 - acc: 0.9337\n",
      "Epoch 74/200\n",
      "34296/34296 [==============================] - 4s 105us/step - loss: 0.1762 - acc: 0.9344ETA: 1s - loss: 0.\n",
      "Epoch 75/200\n",
      "34296/34296 [==============================] - 4s 119us/step - loss: 0.1762 - acc: 0.9339\n",
      "Epoch 76/200\n",
      "34296/34296 [==============================] - 6s 161us/step - loss: 0.1763 - acc: 0.9340\n",
      "Epoch 77/200\n",
      "34296/34296 [==============================] - 6s 182us/step - loss: 0.1763 - acc: 0.9336\n",
      "Epoch 78/200\n",
      "34296/34296 [==============================] - 4s 111us/step - loss: 0.1762 - acc: 0.9343\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1761 - acc: 0.9338\n",
      "Epoch 80/200\n",
      "34296/34296 [==============================] - 2s 56us/step - loss: 0.1762 - acc: 0.9344\n",
      "Epoch 81/200\n",
      "34296/34296 [==============================] - 2s 70us/step - loss: 0.1762 - acc: 0.9336\n",
      "Epoch 82/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1762 - acc: 0.9338: 0s - loss: 0.1711 -\n",
      "Epoch 83/200\n",
      "34296/34296 [==============================] - 2s 55us/step - loss: 0.1763 - acc: 0.9335\n",
      "Epoch 84/200\n",
      "34296/34296 [==============================] - 2s 46us/step - loss: 0.1761 - acc: 0.9345\n",
      "Epoch 85/200\n",
      "34296/34296 [==============================] - 2s 52us/step - loss: 0.1761 - acc: 0.9335\n",
      "Epoch 86/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1760 - acc: 0.9337\n",
      "Epoch 87/200\n",
      "34296/34296 [==============================] - 2s 48us/step - loss: 0.1763 - acc: 0.9334\n",
      "Epoch 88/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1759 - acc: 0.9339\n",
      "Epoch 89/200\n",
      "34296/34296 [==============================] - 2s 57us/step - loss: 0.1760 - acc: 0.9335\n",
      "Epoch 90/200\n",
      "34296/34296 [==============================] - 2s 59us/step - loss: 0.1761 - acc: 0.9334\n",
      "Epoch 91/200\n",
      "34296/34296 [==============================] - 2s 54us/step - loss: 0.1761 - acc: 0.9348\n",
      "Epoch 92/200\n",
      "34296/34296 [==============================] - 2s 69us/step - loss: 0.1761 - acc: 0.9338\n",
      "Epoch 93/200\n",
      "34296/34296 [==============================] - 2s 59us/step - loss: 0.1758 - acc: 0.9332\n",
      "Epoch 94/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1760 - acc: 0.9341\n",
      "Epoch 95/200\n",
      "34296/34296 [==============================] - 2s 60us/step - loss: 0.1758 - acc: 0.9342\n",
      "Epoch 96/200\n",
      "34296/34296 [==============================] - 2s 60us/step - loss: 0.1759 - acc: 0.9338\n",
      "Epoch 97/200\n",
      "34296/34296 [==============================] - 2s 51us/step - loss: 0.1759 - acc: 0.9341\n",
      "Epoch 98/200\n",
      "34296/34296 [==============================] - 2s 48us/step - loss: 0.1760 - acc: 0.9335\n",
      "Epoch 99/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1763 - acc: 0.9338\n",
      "Epoch 100/200\n",
      "34296/34296 [==============================] - 2s 48us/step - loss: 0.1758 - acc: 0.9333\n",
      "Epoch 101/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1756 - acc: 0.9340\n",
      "Epoch 102/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1760 - acc: 0.9341\n",
      "Epoch 103/200\n",
      "34296/34296 [==============================] - 2s 49us/step - loss: 0.1757 - acc: 0.9341\n",
      "Epoch 104/200\n",
      "34296/34296 [==============================] - 2s 48us/step - loss: 0.1758 - acc: 0.9340\n",
      "Epoch 105/200\n",
      "34296/34296 [==============================] - 2s 48us/step - loss: 0.1758 - acc: 0.9340\n",
      "Epoch 106/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1756 - acc: 0.9341\n",
      "Epoch 107/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1759 - acc: 0.9342\n",
      "Epoch 108/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1760 - acc: 0.9340\n",
      "Epoch 109/200\n",
      "34296/34296 [==============================] - 2s 48us/step - loss: 0.1759 - acc: 0.9335\n",
      "Epoch 110/200\n",
      "34296/34296 [==============================] - 2s 70us/step - loss: 0.1759 - acc: 0.9339\n",
      "Epoch 111/200\n",
      "34296/34296 [==============================] - 2s 56us/step - loss: 0.1758 - acc: 0.9334\n",
      "Epoch 112/200\n",
      "34296/34296 [==============================] - 2s 58us/step - loss: 0.1758 - acc: 0.9338\n",
      "Epoch 113/200\n",
      "34296/34296 [==============================] - 2s 54us/step - loss: 0.1755 - acc: 0.9347\n",
      "Epoch 114/200\n",
      "34296/34296 [==============================] - 2s 49us/step - loss: 0.1758 - acc: 0.9335\n",
      "Epoch 115/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1760 - acc: 0.9343\n",
      "Epoch 116/200\n",
      "34296/34296 [==============================] - 2s 48us/step - loss: 0.1757 - acc: 0.9344\n",
      "Epoch 117/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1757 - acc: 0.9345\n",
      "Epoch 118/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1757 - acc: 0.9340\n",
      "Epoch 119/200\n",
      "34296/34296 [==============================] - 2s 54us/step - loss: 0.1757 - acc: 0.9340\n",
      "Epoch 120/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1757 - acc: 0.9338\n",
      "Epoch 121/200\n",
      "34296/34296 [==============================] - 2s 65us/step - loss: 0.1752 - acc: 0.9345: 0s - loss: 0.1753 - acc\n",
      "Epoch 122/200\n",
      "34296/34296 [==============================] - 2s 60us/step - loss: 0.1758 - acc: 0.9345: 1s - loss: 0.1782 - acc: 0.934 - ETA: 1s - loss\n",
      "Epoch 123/200\n",
      "34296/34296 [==============================] - 2s 69us/step - loss: 0.1756 - acc: 0.9341\n",
      "Epoch 124/200\n",
      "34296/34296 [==============================] - 2s 60us/step - loss: 0.1754 - acc: 0.9340\n",
      "Epoch 125/200\n",
      "34296/34296 [==============================] - 2s 57us/step - loss: 0.1755 - acc: 0.9345: 0s - loss: 0.17\n",
      "Epoch 126/200\n",
      "34296/34296 [==============================] - 2s 62us/step - loss: 0.1755 - acc: 0.9344: 2s\n",
      "Epoch 127/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1755 - acc: 0.9345\n",
      "Epoch 128/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1755 - acc: 0.9348\n",
      "Epoch 129/200\n",
      "34296/34296 [==============================] - 2s 58us/step - loss: 0.1756 - acc: 0.9346\n",
      "Epoch 130/200\n",
      "34296/34296 [==============================] - 2s 51us/step - loss: 0.1754 - acc: 0.9345\n",
      "Epoch 131/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1754 - acc: 0.9340\n",
      "Epoch 132/200\n",
      "34296/34296 [==============================] - 2s 51us/step - loss: 0.1753 - acc: 0.9347\n",
      "Epoch 133/200\n",
      "34296/34296 [==============================] - 2s 54us/step - loss: 0.1753 - acc: 0.9339\n",
      "Epoch 134/200\n",
      "34296/34296 [==============================] - 2s 61us/step - loss: 0.1757 - acc: 0.9337\n",
      "Epoch 135/200\n",
      "34296/34296 [==============================] - 2s 57us/step - loss: 0.1753 - acc: 0.9342\n",
      "Epoch 136/200\n",
      "34296/34296 [==============================] - 2s 66us/step - loss: 0.1755 - acc: 0.9346: 0s - loss: 0.1\n",
      "Epoch 137/200\n",
      "34296/34296 [==============================] - 2s 68us/step - loss: 0.1754 - acc: 0.9344\n",
      "Epoch 138/200\n",
      "34296/34296 [==============================] - 3s 77us/step - loss: 0.1753 - acc: 0.9347\n",
      "Epoch 139/200\n",
      "34296/34296 [==============================] - 2s 70us/step - loss: 0.1754 - acc: 0.9342\n",
      "Epoch 140/200\n",
      "34296/34296 [==============================] - 3s 98us/step - loss: 0.1754 - acc: 0.9340\n",
      "Epoch 141/200\n",
      "34296/34296 [==============================] - 2s 65us/step - loss: 0.1753 - acc: 0.9345\n",
      "Epoch 142/200\n",
      "34296/34296 [==============================] - 3s 77us/step - loss: 0.1754 - acc: 0.9342: 0s - loss: 0.1754 - acc: 0.934\n",
      "Epoch 143/200\n",
      "34296/34296 [==============================] - 2s 72us/step - loss: 0.1753 - acc: 0.9345\n",
      "Epoch 144/200\n",
      "34296/34296 [==============================] - 2s 61us/step - loss: 0.1751 - acc: 0.9346: 1s - loss:\n",
      "Epoch 145/200\n",
      "34296/34296 [==============================] - 2s 63us/step - loss: 0.1751 - acc: 0.9341\n",
      "Epoch 146/200\n",
      "34296/34296 [==============================] - 2s 71us/step - loss: 0.1753 - acc: 0.9342\n",
      "Epoch 147/200\n",
      "34296/34296 [==============================] - 3s 86us/step - loss: 0.1753 - acc: 0.9342: 1s -\n",
      "Epoch 148/200\n",
      "34296/34296 [==============================] - 2s 64us/step - loss: 0.1752 - acc: 0.9348: 0s - loss: 0.1775 -\n",
      "Epoch 149/200\n",
      "34296/34296 [==============================] - 2s 48us/step - loss: 0.1751 - acc: 0.9346\n",
      "Epoch 150/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1752 - acc: 0.9339\n",
      "Epoch 151/200\n",
      "34296/34296 [==============================] - 3s 77us/step - loss: 0.1753 - acc: 0.9338: 0s - loss: 0.1754 -\n",
      "Epoch 152/200\n",
      "34296/34296 [==============================] - 2s 56us/step - loss: 0.1751 - acc: 0.9340\n",
      "Epoch 153/200\n",
      "34296/34296 [==============================] - 2s 46us/step - loss: 0.1752 - acc: 0.9344\n",
      "Epoch 154/200\n",
      "34296/34296 [==============================] - 2s 64us/step - loss: 0.1751 - acc: 0.9344\n",
      "Epoch 155/200\n",
      "34296/34296 [==============================] - 3s 75us/step - loss: 0.1753 - acc: 0.9347\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34296/34296 [==============================] - 2s 64us/step - loss: 0.1751 - acc: 0.9343\n",
      "Epoch 157/200\n",
      "34296/34296 [==============================] - 2s 47us/step - loss: 0.1752 - acc: 0.9342: 1s - \n",
      "Epoch 158/200\n",
      "34296/34296 [==============================] - 2s 46us/step - loss: 0.1750 - acc: 0.9347\n",
      "Epoch 159/200\n",
      "34296/34296 [==============================] - 2s 49us/step - loss: 0.1750 - acc: 0.9340\n",
      "Epoch 160/200\n",
      "34296/34296 [==============================] - 2s 70us/step - loss: 0.1749 - acc: 0.9341\n",
      "Epoch 161/200\n",
      "34296/34296 [==============================] - 3s 89us/step - loss: 0.1751 - acc: 0.9334\n",
      "Epoch 162/200\n",
      "34296/34296 [==============================] - 2s 66us/step - loss: 0.1751 - acc: 0.9344\n",
      "Epoch 163/200\n",
      "34296/34296 [==============================] - 2s 61us/step - loss: 0.1750 - acc: 0.9344\n",
      "Epoch 164/200\n",
      "34296/34296 [==============================] - 2s 52us/step - loss: 0.1750 - acc: 0.9347\n",
      "Epoch 165/200\n",
      "34296/34296 [==============================] - 2s 63us/step - loss: 0.1752 - acc: 0.9341\n",
      "Epoch 166/200\n",
      "34296/34296 [==============================] - 2s 63us/step - loss: 0.1752 - acc: 0.9343\n",
      "Epoch 167/200\n",
      "34296/34296 [==============================] - 2s 54us/step - loss: 0.1749 - acc: 0.9347\n",
      "Epoch 168/200\n",
      "34296/34296 [==============================] - 2s 51us/step - loss: 0.1748 - acc: 0.9347\n",
      "Epoch 169/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1750 - acc: 0.9350\n",
      "Epoch 170/200\n",
      "34296/34296 [==============================] - 2s 61us/step - loss: 0.1750 - acc: 0.9340\n",
      "Epoch 171/200\n",
      "34296/34296 [==============================] - 2s 61us/step - loss: 0.1749 - acc: 0.9342: 0s - loss: 0.177\n",
      "Epoch 172/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1750 - acc: 0.9345\n",
      "Epoch 173/200\n",
      "34296/34296 [==============================] - 2s 51us/step - loss: 0.1749 - acc: 0.9351\n",
      "Epoch 174/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1749 - acc: 0.9343\n",
      "Epoch 175/200\n",
      "34296/34296 [==============================] - 2s 56us/step - loss: 0.1749 - acc: 0.9340\n",
      "Epoch 176/200\n",
      "34296/34296 [==============================] - 3s 76us/step - loss: 0.1749 - acc: 0.9339\n",
      "Epoch 177/200\n",
      "34296/34296 [==============================] - 3s 83us/step - loss: 0.1749 - acc: 0.9340\n",
      "Epoch 178/200\n",
      "34296/34296 [==============================] - 3s 80us/step - loss: 0.1749 - acc: 0.9344\n",
      "Epoch 179/200\n",
      "34296/34296 [==============================] - 4s 119us/step - loss: 0.1748 - acc: 0.9345\n",
      "Epoch 180/200\n",
      "34296/34296 [==============================] - 3s 84us/step - loss: 0.1750 - acc: 0.9348\n",
      "Epoch 181/200\n",
      "34296/34296 [==============================] - 2s 70us/step - loss: 0.1747 - acc: 0.9347\n",
      "Epoch 182/200\n",
      "34296/34296 [==============================] - 3s 87us/step - loss: 0.1748 - acc: 0.9346: 0s - loss: 0.1757\n",
      "Epoch 183/200\n",
      "34296/34296 [==============================] - 2s 73us/step - loss: 0.1748 - acc: 0.9340\n",
      "Epoch 184/200\n",
      "34296/34296 [==============================] - 2s 52us/step - loss: 0.1749 - acc: 0.9344\n",
      "Epoch 185/200\n",
      "34296/34296 [==============================] - 2s 50us/step - loss: 0.1750 - acc: 0.9344: 0s - loss: 0.1\n",
      "Epoch 186/200\n",
      "34296/34296 [==============================] - 2s 66us/step - loss: 0.1746 - acc: 0.9342: 0s - los\n",
      "Epoch 187/200\n",
      "34296/34296 [==============================] - 2s 64us/step - loss: 0.1748 - acc: 0.9346\n",
      "Epoch 188/200\n",
      "34296/34296 [==============================] - 4s 102us/step - loss: 0.1746 - acc: 0.9345\n",
      "Epoch 189/200\n",
      "34296/34296 [==============================] - 2s 57us/step - loss: 0.1749 - acc: 0.9346: 1s\n",
      "Epoch 190/200\n",
      "34296/34296 [==============================] - 2s 54us/step - loss: 0.1746 - acc: 0.9348: 0s - loss: 0.1682 \n",
      "Epoch 191/200\n",
      "34296/34296 [==============================] - 2s 68us/step - loss: 0.1746 - acc: 0.9342\n",
      "Epoch 192/200\n",
      "34296/34296 [==============================] - 4s 106us/step - loss: 0.1745 - acc: 0.9344 1s - loss: 0.1758 \n",
      "Epoch 193/200\n",
      "34296/34296 [==============================] - 3s 77us/step - loss: 0.1746 - acc: 0.9344\n",
      "Epoch 194/200\n",
      "34296/34296 [==============================] - 2s 53us/step - loss: 0.1747 - acc: 0.9346\n",
      "Epoch 195/200\n",
      "34296/34296 [==============================] - 2s 67us/step - loss: 0.1745 - acc: 0.9350\n",
      "Epoch 196/200\n",
      "34296/34296 [==============================] - 2s 61us/step - loss: 0.1746 - acc: 0.9344\n",
      "Epoch 197/200\n",
      "34296/34296 [==============================] - 2s 51us/step - loss: 0.1745 - acc: 0.9350\n",
      "Epoch 198/200\n",
      "34296/34296 [==============================] - 3s 95us/step - loss: 0.1749 - acc: 0.9341\n",
      "Epoch 199/200\n",
      "34296/34296 [==============================] - 5s 144us/step - loss: 0.1744 - acc: 0.9351\n",
      "Epoch 200/200\n",
      "34296/34296 [==============================] - 3s 92us/step - loss: 0.1747 - acc: 0.9354\n",
      "[[0.00941902]\n",
      " [0.00766138]\n",
      " [0.03604845]\n",
      " ...\n",
      " [0.00668374]\n",
      " [0.05216768]\n",
      " [0.01290397]]\n",
      "8522/8522 [==============================] - 0s 51us/step\n",
      "0.17903443865916926 0.9299460220605492\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=200)\n",
    "classifications = model.predict(x_test)\n",
    "print(classifications)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "y_predict_probs = model.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "y_predict_classes = model.predict_classes(x_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.set_printoptions(threshold=np.inf)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.929946\n",
      "Precision: 0.900595\n",
      "Recall: 0.778693\n",
      "F1 score: 0.835219\n",
      "Cohens kappa: 0.791034\n",
      "ROC AUC: 0.959724\n",
      "Confusion_matrix:\n",
      "                    TrueP FalseN\n",
      "                    FalseP TrueN\n",
      "[[1513  430]\n",
      " [ 167 6412]]\n"
     ]
    }
   ],
   "source": [
    "# reduce to 1d array\n",
    "y_probs = y_predict_probs[:, 0]\n",
    "y_classes = y_predict_classes[:, 0]\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, y_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, y_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, y_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, y_classes)\n",
    "print('F1 score: %f' % f1)\n",
    "# kappa\n",
    "kappa = cohen_kappa_score(y_test, y_classes)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "# ROC AUC\n",
    "auc = roc_auc_score(y_test, y_probs)\n",
    "print('ROC AUC: %f' % auc)\n",
    "# confusion matrix\n",
    "print('Confusion_matrix:')\n",
    "print('                    TrueP FalseN')\n",
    "print('                    FalseP TrueN')\n",
    "#labels must be specified as [1,0]. Without it, confusion_matrix\n",
    "#will report negative on the 1st row then positives on the 2nd row\n",
    "#note: predictions are in columns and actual values in rows !!\n",
    "matrix = confusion_matrix(y_test, y_classes, labels=[1, 0])\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_probs)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "# plot ROC\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'm', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'b--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
